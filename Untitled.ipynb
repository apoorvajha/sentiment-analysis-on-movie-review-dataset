{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_path=\"/home/apoorva/Documents/kaggle/movie_reviews/train.tsv\"\n",
    "\n",
    "train = pd.read_csv(data_path, header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "\n",
    "print(train)\n",
    "\n",
    "train.head()\n",
    "\n",
    "train.shape\n",
    "\n",
    "\n",
    "trainfull=train['Phrase'].values\n",
    "print (trainfull)\n",
    "\n",
    "trainfull.reshape(156060,1)\n",
    "print(trainfull[0])\n",
    "raw_train=trainfull[1:128001]\n",
    "print (raw_train.shape)\n",
    "test_phrase=trainfull[128001:156033]\n",
    "print(test_phrase)\n",
    "print(test_phrase.shape)\n",
    "\n",
    "Sentiment_trainfull1=train['Sentiment'].values\n",
    "Sentiment_trainfull=np.zeros([156060,5])\n",
    "print (Sentiment_trainfull)\n",
    "\n",
    "for i in range(0,156059):\n",
    "    a=Sentiment_trainfull1[i]\n",
    "    Sentiment_trainfull[i][a]=1\n",
    "print (Sentiment_trainfull)\n",
    "    \n",
    "    \n",
    "\n",
    "Sentiment_train=Sentiment_trainfull[1:128001]\n",
    "Sentiment_test=Sentiment_trainfull[128001:156033]\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_words(raw_phrase):\n",
    "    raw_phrase_notag=BeautifulSoup(raw_phrase,'lxml').get_text()\n",
    "    raw_phrase_letter=re.sub(\"[^a-zA-Z]\",\" \",raw_phrase_notag)\n",
    "    raw_phrase_nocaps=raw_phrase_letter.lower()\n",
    "    raw_phrase_tokens=word_tokenize(raw_phrase_nocaps)\n",
    "    #raw_phrase_nostopword=[word for word in raw_phrase_tokens if word not in stop_words]\n",
    "         \n",
    "    return (\" \".join(raw_phrase_tokens))\n",
    "def count_seqlen(phrase):\n",
    "    phrase_tokens=word_tokenize(phrase)\n",
    "    return (len(phrase_tokens))\n",
    "    \n",
    "\n",
    "clean_train=[]\n",
    "seqlen=[]\n",
    "train_seqlen=[]\n",
    "size=raw_train.size\n",
    "for i in range(0,size):\n",
    "    \n",
    "    clean_train.append(clean_words(raw_train[i]))\n",
    "    seqlen.append(count_seqlen(clean_train[i]))\n",
    "    train_seqlen.append(count_seqlen(clean_train[i]))\n",
    "\n",
    "   \n",
    "print (clean_train[6767])   \n",
    "print (seqlen[6767])\n",
    "\n",
    "sizetest=test_phrase.size\n",
    "test_seqlen=[]\n",
    "for i in range(0,sizetest):\n",
    "    test_seqlen.append(count_seqlen(test_phrase[i]))\n",
    "max_seqlen_train=np.amax(test_seqlen)\n",
    "max_seqlen_test=np.amax(test_seqlen)\n",
    "if max_seqlen_train>max_seqlen_test:\n",
    "    max_seqlen=max_seqlen_train\n",
    "else:\n",
    "    max_seqlen=max_seqlen_test\n",
    "print (max_seqlen)\n",
    "\n",
    "wordsList = np.load('../input/wordsList.npy')\n",
    "\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('../input/wordVectors.npy')\n",
    "\n",
    "def create_id_matrix(clean_train,size,max_seqlen):\n",
    "    id_matrix=np.zeros((size,max_seqlen),dtype='int32')\n",
    "    for phrase_count in range(0,size):\n",
    "        tokens=word_tokenize(clean_train[phrase_count])\n",
    "        word_count=0\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                id_matrix[phrase_count][word_count]=wordsList.index(word)\n",
    "            except ValueError:\n",
    "                id_matrix[phrase_count][word_count] = 399999\n",
    "            word_count=word_count+1\n",
    "    return (id_matrix)\n",
    "            \n",
    "\n",
    "size_test=test_phrase.size\n",
    "\n",
    "\n",
    "id_matrix=create_id_matrix(clean_train,size,max_seqlen)\n",
    "\n",
    "\n",
    "print (id_matrix)\n",
    "id_matrix=np.asarray(id_matrix)\n",
    "print (id_matrix)\n",
    "\n",
    "id_matrix_test=create_id_matrix(test_phrase,size_test,max_seqlen)\n",
    "\n",
    "\n",
    "print(id_matrix.shape)\n",
    "\n",
    "#id_matrix_train=id_matrix[1:62424]\n",
    "#id_matrix_validate=id_matrix[62425:78029]\n",
    "\n",
    "#print (id_matrix_train.shape)\n",
    "\n",
    "#print (id_matrix_test.shape)\n",
    "\n",
    "#print (id_matrix_validate.shape)\n",
    "\n",
    "np.save('idsMatrix1', id_matrix)\n",
    "np.save('idsMatrix', id_matrix_test)\n",
    "\n",
    "id_matrix=np.load('idsMatrix1.npy')\n",
    "id_matrix_test=np.load('idsMatrix.npy')\n",
    "\n",
    "id_matrix[6767][1]\n",
    "\n",
    "#id_matrix_train=id_matrix[1:78030]\n",
    "#id_matrix_test=id_matrix[78030:]\n",
    "\n",
    "batchSize = 64\n",
    "lstmUnits = 64\n",
    "numClasses = 5\n",
    "\n",
    "max_seqlen_test=np.amax(test_seqlen)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, max_seqlen_test])\n",
    "\n",
    "inp=tf.placeholder(tf.int32,[28059,max_seqlen_test])\n",
    "out=tf.placeholder(tf.float32,[28059,numClasses])\n",
    "\n",
    "print (wordVectors.shape)\n",
    "len(wordsList)\n",
    "\n",
    "numDimensions=50\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, max_seqlen, numDimensions]))\n",
    "\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "def Trainbatch(i,batch_size,inp,out):\n",
    "    #print (i)\n",
    "    a=(i-1)*batch_size\n",
    "    #print (a)\n",
    "    b =(i*batch_size)\n",
    "    #print(b)\n",
    "    y=tf.zeros([batch_size,numClasses])\n",
    "    x=tf.zeros([batch_size,max_seqlen])\n",
    "    x=inp[a:b]\n",
    "    #print (x.shape)\n",
    "    y=out[a:b]\n",
    "    #print (y.shape)\n",
    "    #print (y)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "iterations = 1000\n",
    "counter=0\n",
    "#128000/64(batchsiz)=2000\n",
    "for i in range(iterations):\n",
    "    for j in range(1,2000):\n",
    "    \n",
    "        nextBatch, nextBatchLabels= Trainbatch(j,batchSize,id_matrix,Sentiment_train);\n",
    "        sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        print(i,j)\n",
    "   \n",
    "\n",
    "\n",
    "'''#def getValidateBatch(counter):\n",
    "#    labels=np.zeros([batchSize,numClasses])\n",
    " #   arr = np.zeros([batchSize, max_seqlen])\n",
    "  #  for i in range(counter,(counter+batchSize)):\n",
    "   #     arr[i] = id_matrix_validate[i]\n",
    "    #    value=Sentiment_test[i]\n",
    "     #   labels[i][value-1]=1\n",
    "      #  counter=counter+1\n",
    "    #return arr, labels,counter\n",
    "    \n",
    "    \n",
    "def Trainbatch(i,batch_size,inp,out):\n",
    "    #print (i)\n",
    "    a=(i-1)*batch_size\n",
    "    #print (a)\n",
    "    b =(i*batch_size)\n",
    "    #print(b)\n",
    "    x=tf.zeros([batch_size,3])\n",
    "    y=tf.zeros([batch_size,1])\n",
    "    x=inp[a:b]\n",
    "    #print (x.shape)\n",
    "    y=out[a:b]\n",
    "    #print (y.shape)\n",
    "    return x,y\n",
    "'''\n",
    "\n",
    "def Testbatch(i,batch_size,inp,out):\n",
    "    #print (i)\n",
    "    a=(i-1)*batch_size\n",
    "    #print (a)\n",
    "    b =(i*batch_size)\n",
    "    #print(b)\n",
    "    y=tf.zeros([batch_size,numClasses])\n",
    "    x=tf.zeros([batch_size,max_seqlen])\n",
    "    x=inp[a:b]\n",
    "    #print (x.shape)\n",
    "    y=out[a:b]\n",
    "    #print (y.shape)\n",
    "    #print (y)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "iteration = 5\n",
    "total=0\n",
    "for i in range(iteration):\n",
    "    for j in range(1,438):\n",
    "          \n",
    "        nextBatch, nextBatchLabels = Testbatch(j,batchSize,id_matrix_test,Sentiment_test)\n",
    "        acc=(sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100\n",
    "        total=(total+acc)\n",
    "        print (i,j)\n",
    "        total=total/438\n",
    "    print(\"Accuracy for this batch:\", total)\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
